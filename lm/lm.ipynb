{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Modeling Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Task : Given a sequence of words, predict the next word\n",
    "  - Models the probability of sentences in a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 7957\n",
      "Example words: ['嗥', '嗫', '嗷', '嗽', '嗾', '嘇', '嘈', '嘉', '嘎', '嘏', '嘐', '嘒', '嘕', '嘘', '嘛', '嘤', '嘬', '嘱', '嘲', '嘴']\n",
      "Words: 4888154\n"
     ]
    }
   ],
   "source": [
    "words = open('./poem.txt').read().replace('\\n', '_')\n",
    "words_as_set = set(words)\n",
    "id_to_word = sorted(words_as_set)\n",
    "word_to_id = {w: i for i, w in enumerate(id_to_word)}\n",
    "data = [word_to_id[w] for w in words]\n",
    "print('Number of words %d' % len(id_to_word))\n",
    "print('Example words: %s' % id_to_word[1000:1020])\n",
    "print('Words: %d' % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's build the following model\n",
    "  - A recurrent neural network, unrolled in time\n",
    "  - Long short term memory (LSTM) cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='data/lstm.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* LSTM Cell\n",
    "  - Takes input, previous output and current state, and produces output and next state.\n",
    "  \n",
    "$$\n",
    "h_t, C_t = lstm(x_t, h_{t-1}, C_{t-1})\n",
    "$$\n",
    "\n",
    "<img src='data/lstm_cell.png' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Full set of equations ($[]$ is vector concatenation, $\\times$ is matrix multiply, $*$ is element-wise multiply)\n",
    "\n",
    "$$ X = [h_{t-1}, x_t] $$\n",
    "$$ f_t = \\sigma(W_f \\times X + b_f) $$\n",
    "$$ i_t = \\sigma(W_i \\times X + b_i) $$\n",
    "$$ o_t = \\sigma(W_o \\times X + b_o) $$\n",
    "$$ \\tilde{C}_t = tanh(W_C \\times X + b_C) $$\n",
    "$$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "$$ h_t = o_t * tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of the model\n",
    "* We need to pick embedding dimensions and the dimensions of the state vector.\n",
    "  - For convenience, let's pick `embedding_dims = state_size = 128`\n",
    "* Embedding vectors\n",
    "  - `[10000, embedding_dims]`.\n",
    "* The 4 weight matrices in the equation ($W_f, W_i, W_o, W_C$)\n",
    "  - `[2 * state_size, state_size]`\n",
    "* 4 biases ($b_f, b_i, b_o, b_C$)\n",
    "  - `[state_size]`\n",
    "* Softmax classifier logit layer weights and biases\n",
    "  - `[state_size, 10000], [10000]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Implement an LSTM cell as a class, so we can instantiate many layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMCell(object):\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "        self.W_f = tf.Variable(self.initializer())\n",
    "        self.W_i = tf.Variable(self.initializer())\n",
    "        self.W_o = tf.Variable(self.initializer())\n",
    "        self.W_C = tf.Variable(self.initializer())\n",
    "        self.b_f = tf.Variable(tf.zeros([state_size]))\n",
    "        self.b_i = tf.Variable(tf.zeros([state_size]))\n",
    "        self.b_o = tf.Variable(tf.zeros([state_size]))\n",
    "        self.b_C = tf.Variable(tf.zeros([state_size]))\n",
    "        \n",
    "    def __call__(self, x_t, h_t1, C_t1):\n",
    "        X = tf.concat(1, [h_t1, x_t])\n",
    "        f_t = tf.sigmoid(tf.matmul(X, self.W_f) + self.b_f)\n",
    "        i_t = tf.sigmoid(tf.matmul(X, self.W_i) + self.b_i)\n",
    "        o_t = tf.sigmoid(tf.matmul(X, self.W_o) + self.b_o)\n",
    "        Ctilde_t = tf.tanh(tf.matmul(X, self.W_C) + self.b_C)\n",
    "        C_t = f_t * C_t1 + i_t * Ctilde_t\n",
    "        h_t = o_t * tf.tanh(C_t)\n",
    "        return h_t, C_t\n",
    "    \n",
    "    def initializer(self):\n",
    "        return tf.random_uniform([2*self.state_size, self.state_size], -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Declare embedding vectors, LSTM cells, and logit layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NDIMS = 256\n",
    "VOCAB = len(id_to_word)\n",
    "embedding = tf.Variable(tf.random_uniform([VOCAB, NDIMS], -0.02, 0.02))\n",
    "\n",
    "NLAYERS = 4\n",
    "lstm = []\n",
    "for _ in range(NLAYERS):                                                                                                                \n",
    "    lstm.append(LSTMCell(NDIMS))\n",
    "\n",
    "sm_w = tf.Variable(tf.random_uniform([NDIMS, VOCAB], -0.1, 0.1))\n",
    "sm_b = tf.Variable(tf.zeros([VOCAB]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words and targets are placeholders for [batch_size, num_steps]\n",
    "# tensor of word and target ids\n",
    "words = tf.placeholder(tf.int64, name='words')\n",
    "targets = tf.placeholder(tf.int64, name='targets')\n",
    "\n",
    "def model(batch_size, num_steps):\n",
    "    output = [tf.zeros([batch_size, NDIMS])] * NLAYERS\n",
    "    state = [tf.zeros([batch_size, NDIMS])] * NLAYERS\n",
    "    logits = []\n",
    "    preds = []\n",
    "    costs = []\n",
    "    for i in range(num_steps):\n",
    "        # Get the embedding for words\n",
    "        embed = tf.nn.embedding_lookup(embedding, words[:, i])\n",
    "        # Run the LSTM cells\n",
    "        x = embed\n",
    "        for d in range(NLAYERS):\n",
    "            output[d], state[d] = lstm[d](x, output[d], state[d])\n",
    "            x = output[d]\n",
    "        # Get the logits\n",
    "        logits.append(tf.matmul(output[-1], sm_w) + sm_b)\n",
    "        # Get the softmax predictions\n",
    "        preds.append(tf.nn.softmax(logits[-1]))\n",
    "        # Cost per step\n",
    "        costs.append(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits[-1], targets[:, i]))\n",
    "    # Average cost across time steps\n",
    "    cost = tf.reduce_mean(tf.concat(0, costs))\n",
    "    return preds, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Some boring routines to get mini-batch of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input   秦川雄帝宅，函谷壮皇居。绮殿千寻起，离宫\n",
      "Target  川雄帝宅，函谷壮皇居。绮殿千寻起，离宫百\n"
     ]
    }
   ],
   "source": [
    "STEPS=20                                                                                                                          \n",
    "def seq_generator():\n",
    "    curr = 0\n",
    "    while True:                                                                                                                     \n",
    "        if curr > len(data) - STEPS - 1:                                                                                              \n",
    "            curr = 0\n",
    "        w, t = (data[curr:curr + STEPS], data[curr + 1:curr + 1 + STEPS])\n",
    "        curr += STEPS\n",
    "        yield w, t\n",
    "\n",
    "seqgen = seq_generator()\n",
    "\n",
    "w, t = next(seqgen)\n",
    "print(\"Input  \", ''.join([id_to_word[x] for x in w]))\n",
    "print(\"Target \", ''.join([id_to_word[x] for x in t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input   百雉馀。_连薨遥接汉，飞观迥凌虚。云日隐\n",
      "Batch Target  雉馀。_连薨遥接汉，飞观迥凌虚。云日隐层\n",
      "Batch Input   层阙，风烟出绮疏。_岩廊罢机务，崇文聊驻\n",
      "Batch Target  阙，风烟出绮疏。_岩廊罢机务，崇文聊驻辇\n",
      "Batch Input   辇。玉匣启龙图，金绳披凤篆。_韦编断仍续\n",
      "Batch Target  。玉匣启龙图，金绳披凤篆。_韦编断仍续，\n",
      "Batch Input   ，缥帙舒还卷。对此乃淹留，欹案观坟典。_\n",
      "Batch Target  缥帙舒还卷。对此乃淹留，欹案观坟典。_移\n"
     ]
    }
   ],
   "source": [
    "def get_batch(batch_size):\n",
    "    input, target = [], []\n",
    "    for _ in range(batch_size):\n",
    "        w, t = next(seqgen)\n",
    "        input.append(w)\n",
    "        target.append(t)\n",
    "    return np.array(input), np.array(target)                                                                                        \n",
    "\n",
    "BATCH_SIZE=4\n",
    "input, target = get_batch(BATCH_SIZE)\n",
    "for i in range(BATCH_SIZE):\n",
    "    print(\"Batch Input  \", ''.join([id_to_word[x] for x in input[i, :]]))\n",
    "    print(\"Batch Target \", ''.join([id_to_word[x] for x in target[i, :]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Everything in working order?\n",
    "* Try to get the predictions for a random example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001]\n"
     ]
    }
   ],
   "source": [
    "preds, cost = model(1, STEPS)\n",
    "tf.initialize_all_variables().run()\n",
    "w, t = get_batch(1)\n",
    "p = preds[0].eval(feed_dict={words: w, targets: t})\n",
    "np.set_printoptions(formatter={'float': lambda x: '%.04f'%x}, threshold=10000)\n",
    "print(p[0][:100])                                                                                                                 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $e^{cost}$ should be approximately VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.98185 7957.35\n"
     ]
    }
   ],
   "source": [
    "c = cost.eval(feed_dict={words: w, targets: t})\n",
    "print(c, np.exp(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's train the model\n",
    "* Let's get fancy\n",
    "  - Clip gradients before applying to parameters\n",
    "  - Use `tf.train.GradientDescentOptimizer` to reduce some boiler plate\n",
    "  - Use exponential decay on the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a variable to hold the step number, but mark it as not trainable \n",
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(learning_rate, batch_size):\n",
    "    _, cost_value = model(batch_size, STEPS)\n",
    "    all_vars = tf.trainable_variables()\n",
    "    grads = tf.gradients(cost_value, all_vars)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "    # Decay the learning rate by 0.8 every 1000 steps\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate, global_step, 1000, 0.8)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # apply_gradients increments the global_step\n",
    "    train_op = optimizer.apply_gradients(zip(grads, all_vars),\n",
    "                                         global_step=global_step)\n",
    "    return cost_value, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* And we are off to the races!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 8.982\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "cost_value, train_op = train(1.0, BATCH_SIZE)\n",
    "tf.initialize_all_variables().run()\n",
    "for step_number in range(1):\n",
    "    w, t = get_batch(BATCH_SIZE)\n",
    "    c, _ = sess.run([cost_value, train_op], feed_dict={words: w, targets: t})\n",
    "    if step_number % 10 == 0:\n",
    "        print('step %d: %.3f' % (step_number, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./lm_params-1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess, './lm_params', global_step=global_step.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's ask the model to generate sentences\n",
    "  - Start off with few words\n",
    "  - Sample from the probability distribution to get the next word\n",
    "  - Remember to feed the cell state back into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, './lm_params-1')\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embedding, words[:, 0])\n",
    "output_in = [tf.zeros([1, NDIMS])] * NLAYERS\n",
    "state_in = [tf.zeros([1, NDIMS])] * NLAYERS\n",
    "output = [0] * NLAYERS\n",
    "state = [0] * NLAYERS\n",
    "# Run the LSTM cells\n",
    "x = embed\n",
    "for d in range(NLAYERS):\n",
    "    output[d], state[d] = lstm[d](x, output_in[d], state_in[d])\n",
    "    x = output[d]\n",
    "# Get the logits\n",
    "logits = tf.matmul(output[-1], sm_w) + sm_b\n",
    "# Get the softmax predictions\n",
    "preds = tf.nn.softmax(logits)\n",
    "\n",
    "def get_sentence(start_words, length):\n",
    "    w = np.array([[word_to_id[start_words[0]]]])\n",
    "    t = sess.run([preds] + output + state, feed_dict={words: w})\n",
    "    sentence = [start_words[0]]\n",
    "    for i in range(length):\n",
    "        if i + 1 < len(start_words):\n",
    "            w[0, 0] = word_to_id[start_words[i+1]]\n",
    "        else:\n",
    "            w[0, 0] = min(VOCAB, np.sum(np.cumsum(t[0]) < np.random.rand()))\n",
    "        sentence.append(id_to_word[w[0, 0]])\n",
    "        feed_dict = dict(\n",
    "            [(output_in[i], t[1+i]) for i in range(NLAYERS)] +\n",
    "            [(state_in[i], t[1+NLAYERS+i]) for i in range(NLAYERS)] +\n",
    "            [(words, w)])\n",
    "    t = sess.run([preds] + output + state, feed_dict=feed_dict)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国 破 山 河 在 ， 城 春 草 木 深 。 感 时 花 溅 泪 ， 意 青 汉 乍 后 ，\n",
      "国 破 山 河 在 ， 城 春 草 木 深 。 感 时 花 溅 泪 ， 姜 梧 趗 裈 旎 劈\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './lm_params-242001')\n",
    "print(get_sentence('国破山河在，城春草木深。感时花溅泪，', 23))\n",
    "# 鹅 鹅 鹅 ， --> 灯 下 寒 残 啼 。 。 掷 飞 作 。 兮 迸 香 檐 支 毛\n",
    "# 一 览 众 山 小 ， --> 事 点 段 树 榜 带 念\n",
    "# 一 览 众 山 小 ， --> 万 镇 曲 家 一 明 夜\n",
    "# 前 不 见 古 人 ， 后 不 见 来 者 。 --> 暮 兴 闲 客 宠 。 思 住 水 。 风 土 骑\n",
    "# 国 破 山 河 在 ， 城 春 草 木 深 。 感 时 花 溅 泪 ， --> 情 君 沽 古 风 ， 。\n",
    "saver.restore(sess, './lm_params-1')\n",
    "print(get_sentence('国破山河在，城春草木深。感时花溅泪，', 23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "* Increase the `state_size`\n",
    "* Train longer, until the cost goes to `~ 1.0`\n",
    "* Have fun with sentence generation!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
