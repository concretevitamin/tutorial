{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Modeling Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Task : Given a sequence of words, predict the next word\n",
    "  - Models the probability of sentences in a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sequence\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "x & = & x_1, x_2, x_3, ..., x_n \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* E.g.,\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "x & = & 明月几时有\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "$$\n",
    "x_1 = 明, x_2 = 月, x_3 = 几, x_4 = 时, x_5 = 有\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Handle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "    \n",
    "    def __init__(self, corpus, batch, steps):\n",
    "        self.batch = batch\n",
    "        self.steps = steps\n",
    "        words = open(corpus, mode='r').read().replace('\\n', '_')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        tf.logging.info('Number of unique chars: %d', len(self.id_to_word))\n",
    "        tf.logging.info('Number of training chars: %d', len(self.data))\n",
    "        self.seqgen = self.seq_generator()\n",
    "\n",
    "    def seq_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            if curr > len(self.data) - self.steps - 1:\n",
    "                curr = 0\n",
    "            start, limit = curr, curr + self.steps\n",
    "            w, t = (self.data[start:limit], self.data[start + 1:limit + 1])\n",
    "            curr = limit\n",
    "            yield w, t\n",
    "\n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            w, t = next(self.seqgen)\n",
    "            input.append(w)\n",
    "            target.append(t)\n",
    "        return np.array(input), np.array(target)\n",
    "    \n",
    "    def to_words(self, ids):\n",
    "        return [self.id_to_word[x] for x in ids]\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "    \n",
    "    def __init__(self, corpus, batch, steps):\n",
    "        self.batch = batch\n",
    "        self.steps = steps\n",
    "        words = open(corpus, mode='r').read().replace('\\n', '_')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        tf.logging.info('Number of unique chars: %d', len(self.id_to_word))\n",
    "        tf.logging.info('Number of training chars: %d', len(self.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def __init__(...):\n",
    "      self.seqgen = self.seq_generator()        \n",
    "\n",
    "    def seq_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            if curr > len(self.data) - self.steps - 1:\n",
    "                curr = 0\n",
    "            start, limit = curr, curr + self.steps\n",
    "            w, t = (self.data[start:limit], self.data[start + 1:limit + 1])\n",
    "            curr = limit\n",
    "            yield w, t\n",
    "\n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            w, t = next(self.seqgen)\n",
    "            input.append(w)\n",
    "            target.append(t)\n",
    "        return np.array(input), np.array(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def to_words(self, ids):\n",
    "        return [self.id_to_word[x] for x in ids]\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['秦', '川', '雄', '帝', '宅', '，', '函', '谷', '壮', '皇']\n",
      "['川', '雄', '帝', '宅', '，', '函', '谷', '壮', '皇', '居']\n"
     ]
    }
   ],
   "source": [
    "data = TrainData('./data/poem.txt', 1, 10)\n",
    "x, y = data.get_batch()\n",
    "print(data.to_words(x[0]))\n",
    "print(data.to_words(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Model: the probability of a sequence\n",
    "$$ p_\\theta(x) = p_\\theta(x_1)p_\\theta(x_2|x_1)p_\\theta(x_3|x_1x_2)...p_\\theta(x_n|x_1x_2...x_{n-1}) $$\n",
    "* $\\theta$ to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Maximum likelihood estimation.\n",
    "$$ \n",
    "\\operatorname*{arg\\,max}_\\theta \\prod_{x\\in D} p_\\theta(x)\n",
    "$$\n",
    "\n",
    "* Equivalent to\n",
    "$$ \n",
    " -\\frac{1}{N}\\operatorname*{arg\\,min}_\\theta \\sum_{x\\in D} log(p_\\theta(x))\n",
    " = -\\frac{1}{N} \\operatorname*{arg\\,min}_\\theta \\sum_{x\\in D} \\sum_i log(p_\\theta(x_i|x_1x_2...x_{i-1}))\n",
    "$$\n",
    "\n",
    "  $D$ is the data set and $N$ is the number of samples in the data set.\n",
    "\n",
    "* Per-word loss term:\n",
    "$$\n",
    "-log(p_\\theta(x_i|x_1x_2...x_{i-1}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's build the following model\n",
    "  - Character embedding\n",
    "  - A recurrent neural network\n",
    "  - Stacked, unrolled in time\n",
    "  - Long short term memory (LSTM) cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='data/lstm.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* LSTM Cell\n",
    "  - Takes input, previous output and current state, and produces output and next state.\n",
    "  \n",
    "$$\n",
    "h_t, C_t = lstm(x_t, h_{t-1}, C_{t-1})\n",
    "$$\n",
    "\n",
    "<img src='data/lstm_cell.png' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Full set of equations ($[]$ is vector concatenation, $\\times$ is matrix multiply, $*$ is element-wise multiply)\n",
    "\n",
    "$$ X = [h_{t-1}, x_t] $$\n",
    "$$ f_t = \\sigma(W_f \\times X + b_f) $$\n",
    "$$ i_t = \\sigma(W_i \\times X + b_i) $$\n",
    "$$ o_t = \\sigma(W_o \\times X + b_o) $$\n",
    "$$ \\tilde{C}_t = tanh(W_C \\times X + b_C) $$\n",
    "$$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "$$ h_t = o_t * tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Implement an LSTM cell as a class, so we can instantiate many layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    \n",
    "    def __init__(self, ith, dims):\n",
    "        self.dims = dims\n",
    "        with tf.name_scope('lstm_%d' % ith):\n",
    "            self.W_f = tf.Variable(self.initializer(), name='wf')\n",
    "            self.W_i = tf.Variable(self.initializer(), name='wi')\n",
    "            self.W_o = tf.Variable(self.initializer(), name='wo')\n",
    "            self.W_C = tf.Variable(self.initializer(), name='wc')\n",
    "            self.b_f = tf.Variable(tf.zeros([dims]), name='bf')\n",
    "            self.b_i = tf.Variable(tf.zeros([dims]), name='bi')\n",
    "            self.b_o = tf.Variable(tf.zeros([dims]), name='bo')\n",
    "            self.b_C = tf.Variable(tf.zeros([dims]), name='bc')\n",
    "\n",
    "    def forward(self, x_t, h_t1, C_t1):\n",
    "        X = tf.concat(1, [h_t1, x_t])\n",
    "        f_t = tf.sigmoid(tf.matmul(X, self.W_f) + self.b_f)\n",
    "        i_t = tf.sigmoid(tf.matmul(X, self.W_i) + self.b_i)\n",
    "        o_t = tf.sigmoid(tf.matmul(X, self.W_o) + self.b_o)\n",
    "        Ctilde_t = tf.tanh(tf.matmul(X, self.W_C) + self.b_C)\n",
    "        C_t = f_t * C_t1 + i_t * Ctilde_t\n",
    "        h_t = o_t * tf.tanh(C_t)\n",
    "        return h_t, C_t\n",
    "\n",
    "    def initializer(self):\n",
    "        return tf.random_uniform([2*self.dims, self.dims], -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's build the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of the model\n",
    "* We need to pick embedding dimensions and the dimensions of the state vector.\n",
    "  - For convenience, let's pick `dims = 256`\n",
    "* Vocab size.\n",
    "  - `data.vocab = 7957`\n",
    "* Embedding vectors\n",
    "  - `[7957, dims]`.\n",
    "* The 4 weight matrices in the equation ($W_f, W_i, W_o, W_C$)\n",
    "  - `[2 * dims, dims]`\n",
    "* 4 biases ($b_f, b_i, b_o, b_C$)\n",
    "  - `[dims]`\n",
    "* Softmax classifier logit layer weights and biases\n",
    "  - `[dims, 7957], [7957]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Declare embedding vectors, LSTM cells, and logit layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, depth, steps, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.depth = depth\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Var\n",
    "            self.embedding = tf.Variable(tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "            self.lstm = []\n",
    "            for i in range(depth):\n",
    "                self.lstm.append(LSTM(i, self.dims))\n",
    "            with tf.name_scope('sm'):\n",
    "                self.sm_w = tf.Variable(tf.random_uniform([dims, vocab], -0.1, 0.1),\n",
    "                                        name='w')\n",
    "                self.sm_b = tf.Variable(tf.zeros([vocab]), name='b')\n",
    "\n",
    "            # Feeds.\n",
    "            self.words = tf.placeholder(tf.int64)\n",
    "            self.targets = tf.placeholder(tf.int64)\n",
    "        \n",
    "            # Define forward.\n",
    "            batch_size = tf.shape(self.words)[:1] \n",
    "            shape = tf.concat(0, [batch_size, [dims]]) \n",
    "            init_zeros = tf.zeros(shape)\n",
    "            h = [init_zeros] * depth\n",
    "            c = [init_zeros] * depth\n",
    "            o = []\n",
    "            \n",
    "            # Unroll LSTMs.\n",
    "            for i in range(steps):\n",
    "                # Get the embedding for words\n",
    "                x = tf.nn.embedding_lookup(self.embedding, self.words[:, i])\n",
    "                for j in range(self.depth):\n",
    "                    h[j], c[j] = self.lstm[j].forward(x, h[j], c[j])\n",
    "                    x = h[j]\n",
    "                o.append(x)\n",
    "                \n",
    "            # Compute the loss.\n",
    "            outputs = tf.reshape(tf.concat(1, o), [-1, dims])\n",
    "            logits = tf.matmul(outputs, self.sm_w) + self.sm_b\n",
    "            costs = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits, tf.reshape(self.targets, [-1]))\n",
    "            self.preds = tf.nn.softmax(logits)\n",
    "            self.loss = tf.reduce_mean(costs)\n",
    "            \n",
    "            # Define training.\n",
    "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            vars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.loss, vars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = optimizer.apply_gradients(\n",
    "                zip(grads, vars), global_step=self.global_step)\n",
    "\n",
    "            # Summary\n",
    "            tf.scalar_summary('loss', self.loss)\n",
    "            self.summary = tf.merge_summary(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    def train(self, data, logdir, total_steps):\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "\n",
    "        # Recover.\n",
    "        latest = tf.train.latest_checkpoint(logdir)\n",
    "        if latest is not None:\n",
    "            tf.logging.info('restore %s', latest)\n",
    "            saver.restore(sess, latest)\n",
    "\n",
    "        steps = sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                saver.save(sess, logdir + '/lm_params', global_step=steps)\n",
    "            w, t = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                print(w)\n",
    "                print(t)\n",
    "                loss, summary = sess.run([self.loss, self.summary],\n",
    "                                         feed_dict={self.words: w, self.targets: t})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                tf.logging.info('step %d: %.3f', steps, loss)\n",
    "            else:\n",
    "                sess.run(self.train_op, feed_dict={self.words: w, self.targets: t})\n",
    "            steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, depth, steps, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.depth = depth\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "\n",
    "        # Var\n",
    "        self.embedding = tf.Variable(tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "        self.lstm = []\n",
    "        for i in range(depth):\n",
    "            self.lstm.append(LSTM(i, self.dims))\n",
    "        with tf.name_scope('sm'):\n",
    "            self.sm_w = tf.Variable(tf.random_uniform([dims, vocab], -0.1, 0.1), name='w')\n",
    "            self.sm_b = tf.Variable(tf.zeros([vocab]), name='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "        # Feeds.\n",
    "        self.words = tf.placeholder(tf.int64)\n",
    "        self.targets = tf.placeholder(tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "        # Define forward.\n",
    "        batch_size = tf.shape(self.words)[:1] \n",
    "        shape = tf.concat(0, [batch_size, [dims]])\n",
    "        init_zeros = tf.zeros(shape)\n",
    "        h = [init_zeros] * depth\n",
    "        c = [init_zeros] * depth\n",
    "        o = []\n",
    "        \n",
    "        # Unroll LSTMs.\n",
    "        for i in range(self.steps):\n",
    "            # Get the embedding for words\n",
    "            x = tf.nn.embedding_lookup(self.embedding, self.words[:, i])\n",
    "            for j in range(self.depth):\n",
    "                h[j], c[j] = self.lstm[j].forward(x, h[j], c[j])\n",
    "                x = h[j]\n",
    "            o.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "        # Compute the loss.\n",
    "        outputs = tf.reshape(tf.concat(1, o), [-1, dims])\n",
    "        logits = tf.matmul(outputs, self.sm_w) + self.sm_b\n",
    "        costs = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits, tf.reshape(self.targets, [-1]))\n",
    "        self.preds = tf.nn.softmax(logits)\n",
    "        self.loss = tf.reduce_mean(costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "        # Define training.\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        vars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self.loss, vars)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, vars), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "        # Summary\n",
    "        tf.scalar_summary('loss', self.loss)\n",
    "        self.summary = tf.merge_summary(tf.get_collection(tf.GraphKeys.SUMMARIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Train the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def train(self, data, logdir, total_steps):\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "\n",
    "        # Recover.\n",
    "        latest = tf.train.latest_checkpoint(logdir)\n",
    "        if latest is not None:\n",
    "            tf.logging.info('restore %s', latest)\n",
    "            saver.restore(sess, latest)\n",
    "\n",
    "        steps = sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                saver.save(sess, logdir + '/lm_params', global_step=steps)\n",
    "            w, t = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                loss, summary = sess.run([self.loss, self.summary],\n",
    "                                         feed_dict={self.words: w, self.targets: t})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                tf.logging.info('step %d: %.3f', steps, loss)\n",
    "            else:\n",
    "                sess.run(self.train_op, feed_dict={self.words: w, self.targets: t})\n",
    "            steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Tensor name \"global_step_2\" not found in checkpoint files ./lm_params-0\n\t [[Node: save_1/restore_slice_5 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/restore_slice_5/tensor_name, save_1/restore_slice_5/shape_and_slice)]]\nCaused by op 'save_1/restore_slice_5', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2815, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-3c04846f81eb>\", line 13, in <module>\n    model.train(data, './', 100)\n  File \"<ipython-input-36-3ffe9ca4cebc>\", line 65, in train\n    saver = tf.train.Saver(tf.all_variables())\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/ops/io_ops.py\", line 202, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 358, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Tensor name \"global_step_2\" not found in checkpoint files ./lm_params-0\n\t [[Node: save_1/restore_slice_5 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/restore_slice_5/tensor_name, save_1/restore_slice_5/shape_and_slice)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3c04846f81eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-3ffe9ca4cebc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, logdir, total_steps)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlatest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'restore %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restore called with invalid save path %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1105\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Tensor name \"global_step_2\" not found in checkpoint files ./lm_params-0\n\t [[Node: save_1/restore_slice_5 = RestoreSlice[dt=DT_INT32, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/restore_slice_5/tensor_name, save_1/restore_slice_5/shape_and_slice)]]\nCaused by op 'save_1/restore_slice_5', defined at:\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2815, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-3c04846f81eb>\", line 13, in <module>\n    model.train(data, './', 100)\n  File \"<ipython-input-36-3ffe9ca4cebc>\", line 65, in train\n    saver = tf.train.Saver(tf.all_variables())\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/ops/io_ops.py\", line 202, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 358, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# Main driver.\n",
    "corpus = './data/poem.txt'\n",
    "batch = 32\n",
    "steps = 20\n",
    "data = TrainData(corpus, batch, steps)\n",
    "\n",
    "dims = 256\n",
    "vocab = data.vocab\n",
    "depth = 4\n",
    "steps = 20\n",
    "lr = 0.5\n",
    "model = Model(dims, data.vocab, depth, steps, lr)\n",
    "model.train(data, './', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Everything in working order?\n",
    "* Try to get the predictions for a random example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001]\n"
     ]
    }
   ],
   "source": [
    "preds, cost = model(1, STEPS)\n",
    "tf.initialize_all_variables().run()\n",
    "w, t = get_batch(1)\n",
    "p = preds[0].eval(feed_dict={words: w, targets: t})\n",
    "np.set_printoptions(formatter={'float': lambda x: '%.04f'%x}, threshold=10000)\n",
    "print(p[0][:100])                                                                                                                 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $e^{cost}$ should be approximately VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.98182 7957.14\n"
     ]
    }
   ],
   "source": [
    "c = cost.eval(feed_dict={words: w, targets: t})\n",
    "print(c, np.exp(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's train the model\n",
    "* Let's get fancy\n",
    "  - Clip gradients before applying to parameters\n",
    "  - Use `tf.train.GradientDescentOptimizer` to reduce some boiler plate\n",
    "  - Use exponential decay on the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a variable to hold the step number, but mark it as not trainable \n",
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(learning_rate, batch_size):\n",
    "    _, cost_value = model(batch_size, STEPS)\n",
    "    all_vars = tf.trainable_variables()\n",
    "    grads = tf.gradients(cost_value, all_vars)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "    # Decay the learning rate by 0.8 every 1000 steps\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate, global_step, 1000, 0.8)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # apply_gradients increments the global_step\n",
    "    train_op = optimizer.apply_gradients(zip(grads, all_vars),\n",
    "                                         global_step=global_step)\n",
    "    return cost_value, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* And we are off to the races!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 8.982\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "cost_value, train_op = train(1.0, BATCH_SIZE)\n",
    "tf.initialize_all_variables().run()\n",
    "for step_number in range(1):\n",
    "    w, t = get_batch(BATCH_SIZE)\n",
    "    c, _ = sess.run([cost_value, train_op], feed_dict={words: w, targets: t})\n",
    "    if step_number % 10 == 0:\n",
    "        print('step %d: %.3f' % (step_number, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4a70673446fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./lm_params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/google/home/zhifengc/tf/lib/python3.4/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m       saver_def = builder.build(\n\u001b[1;32m    839\u001b[0m           \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess, './lm_params', global_step=global_step.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's ask the model to generate sentences\n",
    "  - Start off with few words\n",
    "  - Sample from the probability distribution to get the next word\n",
    "  - Remember to feed the cell state back into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "embed = tf.nn.embedding_lookup(embedding, words[:, 0])\n",
    "output_in = [tf.zeros([1, NDIMS])] * NLAYERS\n",
    "state_in = [tf.zeros([1, NDIMS])] * NLAYERS\n",
    "output = [0] * NLAYERS\n",
    "state = [0] * NLAYERS\n",
    "# Run the LSTM cells\n",
    "x = embed\n",
    "for d in range(NLAYERS):\n",
    "    output[d], state[d] = lstm[d](x, output_in[d], state_in[d])\n",
    "    x = output[d]\n",
    "# Get the logits\n",
    "logits = tf.matmul(output[-1], sm_w) + sm_b\n",
    "# Get the softmax predictions\n",
    "preds = tf.nn.softmax(logits)\n",
    "\n",
    "def get_sentence(start_words, length):\n",
    "    w = np.array([[word_to_id[start_words[0]]]])\n",
    "    t = sess.run([preds] + output + state, feed_dict={words: w})\n",
    "    sentence = [start_words[0]]\n",
    "    for i in range(length):\n",
    "        if i + 1 < len(start_words):\n",
    "            w[0, 0] = word_to_id[start_words[i+1]]\n",
    "        else:\n",
    "            w[0, 0] = min(VOCAB, np.sum(np.cumsum(t[0]) < np.random.rand()))\n",
    "        sentence.append(id_to_word[w[0, 0]])\n",
    "        feed_dict = dict(\n",
    "            [(output_in[i], t[1+i]) for i in range(NLAYERS)] +\n",
    "            [(state_in[i], t[1+NLAYERS+i]) for i in range(NLAYERS)] +\n",
    "            [(words, w)])\n",
    "    t = sess.run([preds] + output + state, feed_dict=feed_dict)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国 破 山 河 在 ， 峨 怳 楗 楂 擢 映 幔\n",
      "慈 母 手 中 线 ， 嵝 曦 瞪 优 飗 儱 獯\n",
      "一 览 众 山 小 ， 瓜 籥 国 寥 徐 筠 抨\n",
      "明 月 几 时 有 ， 曜 见 撷 柱 绐 转 唝\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './lm_params-1')\n",
    "print(get_sentence('国破山河在，', 12))\n",
    "print(get_sentence('慈母手中线，', 12))\n",
    "print(get_sentence('一览众山小，', 12))\n",
    "print(get_sentence('明月几时有，', 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-77621297b244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./lm_params-375000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'国破山河在，'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'慈母手中线，'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'一览众山小，'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'明月几时有，'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './lm_params-375000')\n",
    "print(get_sentence('国破山河在，', 12))\n",
    "print(get_sentence('慈母手中线，', 12))\n",
    "print(get_sentence('一览众山小，', 12))\n",
    "print(get_sentence('明月几时有，', 12))\n",
    "# 鹅 鹅 鹅 ， --> 灯 下 寒 残 啼 。 。 掷 飞 作 。 兮 迸 香 檐 支 毛\n",
    "# 一 览 众 山 小 ， --> 事 点 段 树 榜 带 念\n",
    "# 一 览 众 山 小 ， --> 万 镇 曲 家 一 明 夜\n",
    "# 前 不 见 古 人 ， 后 不 见 来 者 。 --> 暮 兴 闲 客 宠 。 思 住 水 。 风 土 骑\n",
    "# 国 破 山 河 在 ， 城 春 草 木 深 。 感 时 花 溅 泪 ， --> 情 君 沽 古 风 ， 。\n",
    "# saver.restore(sess, './lm_params-1')\n",
    "# print(get_sentence('国破山河在，城春草木深。感时花溅泪，', 23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "* Increase the `state_size`\n",
    "* Train longer, until the cost goes to `~ 1.0`\n",
    "* Have fun with sentence generation!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
