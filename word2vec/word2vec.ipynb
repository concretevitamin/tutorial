{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Represent a word as a vector\n",
    "  - Two related words should have two vectors close by.\n",
    "* Data. \n",
    "  - 全唐诗 + 全宋词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars =  3987351\n",
      "Unique chars =  7955\n",
      "惊雁落虚弦啼猿悲急箭阅赏诚多美于兹乃忘倦\n",
      "[2066, 7208, 5632, 5795, 1858, 954, 3918, 2056, 1981, 4756, 7108, 6383, 6217, 1257, 5122, 171, 492, 141, 1939, 381]\n"
     ]
    }
   ],
   "source": [
    "words = open('./data/poem.txt').read().replace('。', '').replace('，', '').replace('\\n', '')\n",
    "print(\"Total chars = \", len(words))\n",
    "words_as_set = set(words)\n",
    "print(\"Unique chars = \", len(words_as_set))\n",
    "id_to_word = sorted(words_as_set)\n",
    "word_to_id = {w: i for i, w in enumerate(id_to_word)}\n",
    "data = [word_to_id[w] for w in words]\n",
    "print(words[100:120])\n",
    "print(data[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4562 1707\n",
      "4562 7209\n",
      "4562 1746\n",
      "4562 1472\n",
      "4562 552\n",
      "4562 6301\n",
      "4562 1242\n",
      "4562 4228\n",
      "4562 1565\n",
      "4562 5013\n"
     ]
    }
   ],
   "source": [
    "def skipgram_generator(window=8):\n",
    "    curr = 0\n",
    "    while True:\n",
    "        curr %= len(data)\n",
    "        x = data[curr]\n",
    "        left = data[max(0, curr - window):curr]\n",
    "        right = data[curr+1:x+window]\n",
    "        for y in left + right:\n",
    "            yield (x, y)\n",
    "        curr += 1\n",
    "        \n",
    "skipgram = skipgram_generator()\n",
    "\n",
    "for _ in range(10):\n",
    "    x, y = next(skipgram)\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4562, 4562, 4562, 4562], [3175, 689, 1532, 6413])\n",
      "([4562, 4562, 4562, 4562], [4543, 1497, 4224, 7213])\n",
      "([4562, 4562, 4562, 4562], [7458, 6659, 5750, 6730])\n",
      "([4562, 4562, 4562, 4562], [2386, 3229, 7408, 6101])\n",
      "([4562, 4562, 4562, 4562], [6663, 533, 5795, 173])\n",
      "([4562, 4562, 4562, 4562], [2601, 7187, 1563, 7128])\n",
      "([4562, 4562, 4562, 4562], [7396, 3717, 550, 5013])\n",
      "([4562, 4562, 4562, 4562], [4139, 1608, 1820, 5099])\n",
      "([4562, 4562, 4562, 4562], [2748, 630, 1636, 2553])\n",
      "([4562, 4562, 4562, 4562], [5201, 7524, 6608, 3938])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(batchsize=128):\n",
    "    xs, ys = [], []\n",
    "    for _ in range(batchsize):\n",
    "        (x, y) = next(skipgram)\n",
    "        xs += [x]\n",
    "        ys += [y]\n",
    "    return xs, ys\n",
    "\n",
    "for _ in range(10):\n",
    "    print(get_batch(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NDIMS=128\n",
    "NWORDS=len(id_to_word)\n",
    "embedding = tf.Variable(tf.random_uniform([NWORDS, NDIMS], -0.02, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int64)\n",
    "targets = tf.placeholder(tf.int64)\n",
    "\n",
    "def model():\n",
    "    x_emb = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    y_emb = tf.nn.embedding_lookup(embedding, targets)\n",
    "    scores = tf.reduce_sum(x_emb * y_emb, [1])\n",
    "    probs = tf.sigmoid(scores)\n",
    "    logp = tf.log(probs)\n",
    "    mean_logp = tf.reduce_mean(logp)\n",
    "    return -mean_logp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  [0.69313043]\n",
      "loss =  [0.69330263]\n",
      "loss =  [0.69310367]\n"
     ]
    }
   ],
   "source": [
    "loss = model()\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "for _ in range(3):\n",
    "    xs, ys = get_batch()\n",
    "    print(\"loss = \", sess.run([loss], {inputs:xs, targets:ys}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "def train(learning_rate=1.0):\n",
    "    loss = model()\n",
    "    vars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, vars)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, vars), global_step=global_step)\n",
    "    return loss, train_op\n",
    "\n",
    "loss, train_op = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 0.6931\n",
      "step 1000: 0.6924\n"
     ]
    }
   ],
   "source": [
    "tf.initialize_all_variables().run()\n",
    "for steps in range(2000):\n",
    "    xs, ys = get_batch(batchsize=1024)\n",
    "    l, _ = sess.run([loss, train_op], feed_dict={inputs:xs, targets:ys})\n",
    "    if steps % 1000 == 0:\n",
    "        print('step %d: %.4f' % (steps, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/w2v_params-2000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess, './data/w2v_params', global_step=global_step.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(tf.int64)\n",
    "norm_embs = tf.nn.l2_normalize(embedding, 1)\n",
    "word_embs = tf.nn.embedding_lookup(norm_embs, word_ids)\n",
    "dist = tf.matmul(word_embs, norm_embs, transpose_b=True)\n",
    "topk = tf.nn.top_k(dist, k=10)\n",
    "\n",
    "def word_to_ids(words):\n",
    "    ids = []\n",
    "    for w in words:\n",
    "        if w in word_to_id:\n",
    "            ids += [word_to_id[w]]\n",
    "        else:\n",
    "            ids += [word_to_id['。']]\n",
    "    return ids\n",
    "\n",
    "def nearby(words):\n",
    "    dist, ids = sess.run(topk, feed_dict={word_ids:word_to_ids(words)})\n",
    "    for (p, r) in zip(dist, ids):\n",
    "        print([id_to_word[w] for w in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['千', '霰', '瘵', '龃', '琫', '箫', '饪', '髫', '蒜', '鹉']\n",
      "['里', '沌', '具', '镆', '宦', '韵', '畤', '玛', '辩', '鞙']\n",
      "['冰', '畛', '榞', '酱', '怿', '蒻', '饘', '睫', '厦', '鉟']\n",
      "['封', '鲤', '袓', '侬', '罕', '墝', '喙', '陨', '呱', '渑']\n",
      "['万', '譬', '假', '燑', '贽', '荦', '晕', '峣', '鍧', '仕']\n",
      "['里', '沌', '具', '镆', '宦', '韵', '畤', '玛', '辩', '鞙']\n",
      "['雪', '唼', '悺', '筼', '冽', '亚', '臀', '岚', '递', '编']\n",
      "['飘', '櫁', '矮', '潞', '呼', '稙', '绎', '自', '枮', '争']\n"
     ]
    }
   ],
   "source": [
    "tf.initialize_all_variables().run()\n",
    "nearby('千里冰封万里雪飘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['千', '玉', '不', '香', '一', '里', '天', '高', '野', '云']\n",
      "['里', '风', '日', '不', '长', '有', '见', '高', '云', '为']\n",
      "['冰', '雁', '天', '不', '无', '长', '珠', '秋', '还', '为']\n",
      "['封', '秋', '王', '有', '空', '风', '鸟', '水', '一', '马']\n",
      "['万', '人', '风', '一', '不', '云', '香', '飞', '上', '何']\n",
      "['里', '风', '日', '不', '长', '有', '见', '高', '云', '为']\n",
      "['雪', '风', '霜', '上', '长', '不', '中', '飞', '一', '露']\n",
      "['飘', '飞', '高', '阙', '道', '人', '不', '出', '衣', '风']\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './data/w2v_params-100000')\n",
    "nearby('千里冰封万里雪飘')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(tf.int64, shape=[3])\n",
    "norm_embs = tf.nn.l2_normalize(embedding, 1)\n",
    "word_embs = tf.nn.embedding_lookup(norm_embs, word_ids)\n",
    "target = tf.expand_dims(word_embs[1, :] - word_embs[0, :] + word_embs[2, :], 0)\n",
    "dist = tf.matmul(target, norm_embs, transpose_b=True)\n",
    "topk = tf.nn.top_k(dist, k=10)\n",
    "\n",
    "def analogy(a, b, x):\n",
    "    dist, ids = sess.run(topk, feed_dict={word_ids:word_to_ids([a, b, x])})\n",
    "    for (p, r) in zip(dist, ids):\n",
    "        print([id_to_word[w] for w in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['左', '下', '何', '枝', '疏', '无', '见', '玉', '烟', '临']\n"
     ]
    }
   ],
   "source": [
    "analogy('上', '下', '左')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
