{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Represent a word as a vector\n",
    "  - Two related words should have two vectors close by.\n",
    "* Data. \n",
    "  - 100MB wikipedia. Available from http://http://mattmahoney.net/dc/text8.zip\n",
    "  - 全唐诗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars =  1635832\n",
      "Unique chars =  5853\n",
      "直恐好风光，尽随伊归去。\n",
      "[3245, 1581, 1019, 5487, 363, 5846, 1237, 5337, 189, 1473, 606, 38]\n"
     ]
    }
   ],
   "source": [
    "words = open('./data/sc.utf8.cleaned.txt').read().replace('\\n', '').replace(' ', '')\n",
    "print(\"Total chars = \", len(words))\n",
    "words_as_set = set(words)\n",
    "print(\"Unique chars = \", len(words_as_set))\n",
    "id_to_word = sorted(words_as_set)\n",
    "word_to_id = {w: i for i, w in enumerate(id_to_word)}\n",
    "data = [word_to_id[w] for w in words]\n",
    "print(words[2005:2017])\n",
    "print(data[2005:2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5241 1527\n",
      "5241 1144\n",
      "5241 1259\n",
      "5241 5846\n",
      "5241 1259\n",
      "5241 865\n",
      "5241 2732\n",
      "5241 1524\n",
      "5241 1021\n",
      "5241 5778\n"
     ]
    }
   ],
   "source": [
    "def skipgram_generator(window=8):\n",
    "    curr = 0\n",
    "    while True:\n",
    "        curr %= len(data)\n",
    "        x = data[curr]\n",
    "        left = data[max(0, curr - window):curr]\n",
    "        right = data[curr+1:x+window]\n",
    "        for y in left + right:\n",
    "            yield (x, y)\n",
    "        curr += 1\n",
    "        \n",
    "skipgram = skipgram_generator()\n",
    "\n",
    "for _ in range(10):\n",
    "    x, y = next(skipgram)\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([5241, 5241, 5241, 5241], [3600, 38, 343, 1725])\n",
      "([5241, 5241, 5241, 5241], [842, 5407, 656, 2732])\n",
      "([5241, 5241, 5241, 5241], [1445, 38, 4879, 2336])\n",
      "([5241, 5241, 5241, 5241], [606, 4934, 2192, 38])\n",
      "([5241, 5241, 5241, 5241], [4079, 4159, 5545, 790])\n",
      "([5241, 5241, 5241, 5241], [4939, 124, 5268, 38])\n",
      "([5241, 5241, 5241, 5241], [5268, 58, 2705, 965])\n",
      "([5241, 5241, 5241, 5241], [2417, 59, 5176, 38])\n",
      "([5241, 5241, 5241, 5241], [465, 2192, 1224, 863])\n",
      "([5241, 5241, 5241, 5241], [2523, 150, 4447, 38])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(batchsize=128):\n",
    "    xs, ys = [], []\n",
    "    for _ in range(batchsize):\n",
    "        (x, y) = next(skipgram)\n",
    "        xs += [x]\n",
    "        ys += [y]\n",
    "    return xs, ys\n",
    "\n",
    "for _ in range(10):\n",
    "    print(get_batch(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NDIMS=128\n",
    "NWORDS=len(id_to_word)\n",
    "embedding = tf.Variable(tf.random_uniform([NWORDS, NDIMS], -0.02, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int64)\n",
    "targets = tf.placeholder(tf.int64)\n",
    "\n",
    "def model():\n",
    "    x_emb = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    y_emb = tf.nn.embedding_lookup(embedding, targets)\n",
    "    scores = tf.reduce_sum(x_emb * y_emb, [1])\n",
    "    probs = tf.sigmoid(scores)\n",
    "    logp = tf.log(probs)\n",
    "    mean_logp = tf.reduce_mean(logp)\n",
    "    return -mean_logp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  [0.69295943]\n",
      "loss =  [0.69311172]\n",
      "loss =  [0.69327652]\n"
     ]
    }
   ],
   "source": [
    "loss = model()\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "for _ in range(3):\n",
    "    xs, ys = get_batch()\n",
    "    print(\"loss = \", sess.run([loss], {inputs:xs, targets:ys}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "def train(learning_rate=1.0):\n",
    "    loss = model()\n",
    "    vars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, vars)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, vars), global_step=global_step)\n",
    "    return loss, train_op\n",
    "\n",
    "loss, train_op = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 0.6930\n"
     ]
    }
   ],
   "source": [
    "tf.initialize_all_variables().run()\n",
    "for steps in range(10000):\n",
    "    xs, ys = get_batch(batchsize=1024)\n",
    "    l, _ = sess.run([loss, train_op], feed_dict={inputs:xs, targets:ys})\n",
    "    if steps % 1000 == 0:\n",
    "        print('step %d: %.4f' % (steps, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./w2v_params-1000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess, './w2v_params', global_step=global_step.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(tf.int64)\n",
    "norm_embs = tf.nn.l2_normalize(embedding, 1)\n",
    "word_embs = tf.nn.embedding_lookup(norm_embs, word_ids)\n",
    "dist = tf.matmul(word_embs, norm_embs, transpose_b=True)\n",
    "topk = tf.nn.top_k(dist, k=10)\n",
    "\n",
    "def word_to_ids(words):\n",
    "    ids = []\n",
    "    for w in words:\n",
    "        if w in word_to_id:\n",
    "            ids += [word_to_id[w]]\n",
    "        else:\n",
    "            ids += [word_to_id['。']]\n",
    "    return ids\n",
    "\n",
    "def nearby(words):\n",
    "    dist, ids = sess.run(topk, feed_dict={word_ids:word_to_ids(words)})\n",
    "    for (p, r) in zip(dist, ids):\n",
    "        print([id_to_word[w] for w in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['千', '揎', '沾', '觉', '剜', '疣', '痣', '辋', '缑', '学']\n",
      "['里', '括', '狮', '帻', '匿', '攸', '醱', '呖', '犭', '戚']\n",
      "['冰', '倏', '阻', '胪', '摴', '阙', '籀', '峙', '錝', '淆']\n",
      "['封', '惰', '圳', '犒', '浚', '蚓', '产', '纪', '粪', '胜']\n",
      "['万', '鵷', '共', '枥', '皑', '峥', '汲', '姿', '佛', '补']\n",
      "['里', '括', '狮', '帻', '匿', '攸', '醱', '呖', '犭', '戚']\n",
      "['雪', '堞', '鹠', '铒', '崇', '缵', '乌', '绥', '艗', '邺']\n",
      "['飘', '咨', '粱', '佚', '昏', '俞', '蕤', '酒', '岨', '鼯']\n"
     ]
    }
   ],
   "source": [
    "tf.initialize_all_variables().run()\n",
    "nearby('千里冰封万里雪飘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['千', '，', '、', '。', '长', '风', '年', '不', '是', '花']\n",
      "['里', '，', '长', '。', '、', '花', '雨', '风', '云', '金']\n",
      "['冰', '。', '，', '、', '江', '春', '无', '天', '长', '梦']\n",
      "['封', '，', '酒', '、', '一', '。', '阁', '醉', '风', '阳']\n",
      "['万', '，', '。', '、', '长', '无', '酒', '春', '明', '风']\n",
      "['里', '，', '长', '。', '、', '花', '雨', '风', '云', '金']\n",
      "['雪', '，', '、', '。', '一', '去', '轻', '清', '长', '自']\n",
      "['飘', '，', '。', '年', '、', '相', '云', '何', '天', '无']\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, '../w2v/w2v_params-100000')\n",
    "nearby('千里冰封万里雪飘')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(tf.int64, shape=[3])\n",
    "norm_embs = tf.nn.l2_normalize(embedding, 1)\n",
    "word_embs = tf.nn.embedding_lookup(norm_embs, word_ids)\n",
    "target = tf.expand_dims(word_embs[1, :] - word_embs[0, :] + word_embs[2, :], 0)\n",
    "dist = tf.matmul(target, norm_embs, transpose_b=True)\n",
    "topk = tf.nn.top_k(dist, k=10)\n",
    "\n",
    "def analogy(a, b, x):\n",
    "    dist, ids = sess.run(topk, feed_dict={word_ids:word_to_ids([a, b, x])})\n",
    "    for (p, r) in zip(dist, ids):\n",
    "        print([id_to_word[w] for w in r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['左', '下', '连', '春', '百', '雨', '一', '，', '杨', '。']\n"
     ]
    }
   ],
   "source": [
    "analogy('上', '下', '左')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
