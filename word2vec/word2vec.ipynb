{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Represent a word as a vector\n",
    "  - Two related words should have two vectors close by.\n",
    "* Data. \n",
    "  - 全唐诗 + 全宋词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "\n",
    "    def __init__(self, corpus, batch, windows):\n",
    "        self.batch = batch\n",
    "        self.windows = windows\n",
    "        words = open(corpus, mode='r').read()\n",
    "        words = words.replace('。', '').replace('，', '').replace('\\n', '')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        print('Number of unique chars: ', len(self.id_to_word))\n",
    "        print('Number of training chars: ', len(self.data))\n",
    "        self.seqgen = self.skipgram_generator()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)\n",
    "    \n",
    "    def skipgram_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            curr %= len(self.data)\n",
    "            x = self.data[curr]\n",
    "            left = self.data[max(0, curr - self.windows):curr]\n",
    "            right = self.data[curr + 1:curr + 1 + self.windows]\n",
    "            for y in left + right:\n",
    "                yield (x, y)\n",
    "            curr += 1\n",
    "            \n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            x, y = next(self.seqgen)\n",
    "            input.append(x)\n",
    "            target.append(y)\n",
    "        return np.array(input), np.array(target)\n",
    "    \n",
    "    def to_ids(self, words):\n",
    "        ids = []\n",
    "        for w in words:\n",
    "            if w in self.word_to_id:\n",
    "                ids.append(self.word_to_id[w])\n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "\n",
    "    def __init__(self, corpus, batch, windows):\n",
    "        self.batch = batch\n",
    "        self.windows = windows\n",
    "        words = open(corpus, mode='r').read()\n",
    "        words = words.replace('。', '').replace('，', '').replace('\\n', '')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        print('Number of unique chars: ', len(self.id_to_word))\n",
    "        print('Number of training chars: ', len(self.data))\n",
    "        self.seqgen = self.skipgram_generator()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)\n",
    "    \n",
    "    def to_ids(self, words):\n",
    "        ids = []\n",
    "        for w in words:\n",
    "            if w in self.word_to_id:\n",
    "                ids.append(self.word_to_id[w])\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "    def skipgram_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            curr %= len(self.data)\n",
    "            x = self.data[curr]\n",
    "            left = self.data[max(0, curr - self.windows):curr]\n",
    "            right = self.data[curr + 1:curr + 1 + self.windows]\n",
    "            for y in left + right:\n",
    "                yield (x, y)\n",
    "            curr += 1\n",
    "            \n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            x, y = next(self.seqgen)\n",
    "            input.append(x)\n",
    "            target.append(y)\n",
    "        return np.array(input), np.array(target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars:  7955\n",
      "Number of training chars:  3987351\n"
     ]
    }
   ],
   "source": [
    "data = TrainData('./data/poem.txt', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4562, 4562, 1707, 1707, 1707]), array([1707, 7209, 4562, 7209, 1746]))\n",
      "(array([7209, 7209, 7209, 7209, 1746]), array([4562, 1707, 1746, 1472, 1707]))\n",
      "(array([1746, 1746, 1746, 1472, 1472]), array([7209, 1472,  552, 7209, 1746]))\n",
      "(array([1472, 1472,  552,  552,  552]), array([ 552, 6301, 1746, 1472, 6301]))\n",
      "(array([ 552, 6301, 6301, 6301, 6301]), array([1242, 1472,  552, 1242, 4228]))\n",
      "(array([1242, 1242, 1242, 1242, 4228]), array([ 552, 6301, 4228, 1565, 6301]))\n",
      "(array([4228, 4228, 4228, 1565, 1565]), array([1242, 1565, 5013, 1242, 4228]))\n",
      "(array([1565, 1565, 5013, 5013, 5013]), array([5013, 3175, 4228, 1565, 3175]))\n",
      "(array([5013, 3175, 3175, 3175, 3175]), array([ 689, 1565, 5013,  689, 1532]))\n",
      "(array([ 689,  689,  689,  689, 1532]), array([5013, 3175, 1532, 6413, 3175]))\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(data.get_batch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.lr = lr\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Var\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "\n",
    "            # Feeds.\n",
    "            self.inputs = tf.placeholder(tf.int64)\n",
    "            self.targets = tf.placeholder(tf.int64)\n",
    "\n",
    "            # Define forward.\n",
    "            x_emb = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
    "            y_emb = tf.nn.embedding_lookup(self.embedding, self.targets)\n",
    "\n",
    "            # Compute the loss.\n",
    "            scores = tf.reduce_sum(x_emb * y_emb, [1])\n",
    "            probs = tf.sigmoid(scores)      logp = tf.log(probs)\n",
    "            self.loss = - tf.reduce_mean(logp)\n",
    "\n",
    "            # Define training.\n",
    "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            vars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.loss, vars)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = optimizer.apply_gradients(\n",
    "                zip(grads, vars), global_step=self.global_step)\n",
    "\n",
    "            # Summary\n",
    "            tf.scalar_summary('loss', self.loss)\n",
    "            self.summary = tf.merge_summary(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "            # Inference\n",
    "\n",
    "            # Nearest neighbors\n",
    "            norm_embs = tf.nn.l2_normalize(self.embedding, 1)\n",
    "            word_embs = tf.nn.embedding_lookup(norm_embs, self.inputs)\n",
    "            distance = tf.matmul(word_embs, norm_embs, transpose_b=True)\n",
    "            self.neighbors_topk = tf.nn.top_k(distance, k=10)\n",
    "\n",
    "            # Analogy\n",
    "            a, b, c = word_embs[1, :], word_embs[0, :], word_embs[2, :]\n",
    "            d = b - a + c\n",
    "            target = tf.reshape(d, [1, -1])\n",
    "            dist = tf.matmul(target, norm_embs, transpose_b=True)\n",
    "            self.analogy_topk = tf.nn.top_k(dist, k=10)\n",
    "\n",
    "            # Init\n",
    "            self.init = tf.initialize_all_variables()\n",
    "\n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(tf.all_variables())\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train(self, data, logdir, total_steps):\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "        \n",
    "        # Recover.\n",
    "        self.load(tf.train.latest_checkpoint(logdir))\n",
    "        \n",
    "        steps = self.sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                self.saver.save(\n",
    "                    self.sess, logdir + '/wv_params', global_step=steps)\n",
    "            x, y = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                loss, summary = self.sess.run(\n",
    "                    [self.loss, self.summary],\n",
    "                    feed_dict={self.inputs: x, self.targets: y})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                tf.logging.info('step %d: %.4f', steps, loss)\n",
    "            else:\n",
    "                self.sess.run(\n",
    "                    self.train_op,\n",
    "                    feed_dict={self.inputs: x, self.targets: y})\n",
    "            steps += 1\n",
    "\n",
    "    def load(self, checkpoint):\n",
    "        if checkpoint is not None:\n",
    "            tf.logging.info('restore %s', checkpoint)\n",
    "            self.saver.restore(self.sess, checkpoint)\n",
    "            \n",
    "    def nearby(self, data, words):\n",
    "        ids = data.to_ids(words)\n",
    "        print('ids = %s' % ids)\n",
    "        _, neighbors = self.sess.run(\n",
    "            self.neighbors_topk, feed_dict={self.inputs : ids})\n",
    "        for (w, n) in zip(words, neighbors):\n",
    "            print('nearby  %s --> %s' % (w, ''.join(\n",
    "                        [data.id_to_word[x] for x in n])))\n",
    "            \n",
    "    def analogy(self, data, words):\n",
    "        _, neighbors = self.sess.run(\n",
    "            self.analogy_topk,\n",
    "            feed_dict={self.inputs : data.to_ids(words)})\n",
    "        neighbors = [data.id_to_word[x] for x in neighbors[0, :]]\n",
    "        print('analogy %s %s' % (words, ''.join(neighbors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analogy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
